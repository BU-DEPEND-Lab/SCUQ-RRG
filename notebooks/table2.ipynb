{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/restricted/projectnb/batmanlab/chyuwang/RaDialog/exp/uncertainty/green_uncertainty-3858.pkl', 'rb') as file:\n",
    "    green_uncertainty = pickle.load(file)\n",
    "with open('/restricted/projectnb/batmanlab/chyuwang/RaDialog/exp/text_score/green_scores-3858.pkl', 'rb') as file:\n",
    "    green_score = pickle.load(file)\n",
    "us = np.array([t.numpy() for t in green_score['greens']])\n",
    "\n",
    "\n",
    "ug = np.array([t.numpy() for t in green_uncertainty['uncertainty']])\n",
    "pearson_coeff, _ = pearsonr(us, ug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_empirical_rce(uncertainty_values, correctness_values, num_bins=20):\n",
    "    \"\"\"\n",
    "    Calculate the Empirical Rank Calibration Error (RCE) for uncertainty and correctness values.\n",
    "    \n",
    "    Parameters:\n",
    "    - uncertainty_values: numpy array of uncertainty values (ug)\n",
    "    - correctness_values: numpy array of correctness values (us)\n",
    "    - num_bins: Number of bins to divide the uncertainty values (default is 20)\n",
    "    \n",
    "    Returns:\n",
    "    - empirical_rce: Calculated Empirical Rank Calibration Error\n",
    "    \"\"\"\n",
    "    quantiles = np.linspace(0, 1, num_bins + 1)\n",
    "    bin_edges = np.quantile(uncertainty_values, quantiles)\n",
    "    \n",
    "    bin_indices = np.digitize(uncertainty_values, bin_edges, right=True) - 1  # Bin indices for each uncertainty value\n",
    "\n",
    "    expected_correctness = np.zeros(num_bins)\n",
    "    average_uncertainty = np.zeros(num_bins)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "\n",
    "    for i in range(len(uncertainty_values)):\n",
    "        bin_idx = bin_indices[i]\n",
    "        if 0 <= bin_idx < num_bins:\n",
    "            expected_correctness[bin_idx] += correctness_values[i]\n",
    "            average_uncertainty[bin_idx] += uncertainty_values[i]\n",
    "            bin_counts[bin_idx] += 1\n",
    "\n",
    "    expected_correctness /= np.maximum(bin_counts, 1)\n",
    "    average_uncertainty /= np.maximum(bin_counts, 1)\n",
    "\n",
    "    return expected_correctness,average_uncertainty\n",
    "\n",
    "def cal_rce(uq,score,num_bins):\n",
    "    rce = 0\n",
    "    tmp = calculate_empirical_rce(uq, score, num_bins=20)\n",
    "    acc = tmp[0]\n",
    "    uq = tmp[1]\n",
    "    for i in range(num_bins):\n",
    "        ans = abs(len([bol for bol in acc>acc[i] if bol == True]) - len([bol for bol in uq<uq[i] if bol == True]))/num_bins\n",
    "        rce += ans\n",
    "    return rce/num_bins\n",
    "# Calculate RCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('/restricted/projectnb/batmanlab/chyuwang/rrg_factual_uncertainty/exp_result/RaDialog/scores/report_scores_-1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadCliQ = res['RadCliQ-v0'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = pd.read_csv('/restricted/projectnb/batmanlab/chyuwang/rrg_factual_uncertainty/exp/uncertainty/u_normnll.csv')\n",
    "pes = list(pe['u_normnll'].values)\n",
    "pe1 = pd.read_csv('/restricted/projectnb/batmanlab/chyuwang/rrg_factual_uncertainty/exp/uncertainty/u_nll.csv')\n",
    "pes1 = np.array(pe1['u_nll'].values)\n",
    "u_lexicalsim = pd.read_csv('/restricted/projectnb/batmanlab/chyuwang/rrg_factual_uncertainty/exp_result/RaDialog/UQ/lexicalUQ.csv')\n",
    "u_lexicalsim = u_lexicalsim['ROUGE_L_UQ'].values\n",
    "u_lexicalsim = [1-i for i in u_lexicalsim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_list = [pes,pes1,u_lexicalsim,ug] # Normalized Entropy - Predictive Entropy  - Lexical Similarity - VRO-GREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045 0.145\n",
      "0.045000000000000005 0.09\n",
      "0.045 0.04\n",
      "0.015 0.019999999999999997\n"
     ]
    }
   ],
   "source": [
    "for idx,i in enumerate(uncertainty_list):\n",
    "    print(cal_rce(i,us,num_bins=20),cal_rce(i[:-1],-RadCliQ,num_bins=20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  CheXpertPlus_mimiccxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.read_csv('/restricted/projectnb/batmanlab/chyuwang/rrg_factual_uncertainty/exp_result/ChexpertPlus/cxr_benchmark/chexpertPlus_report_scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.read_csv('/restricted/projectnb/batmanlab/chyuwang/rrg_factual_uncertainty/exp_result/ChexpertPlus/green_scores-chexpert-plus-3858.csv',header=None)\n",
    "ugreen = pd.read_csv('/restricted/projectnb/batmanlab/chyuwang/rrg_factual_uncertainty/exp_result/ChexpertPlus/chexpert-plus-green_uncertainty-3858.csv',header=None)\n",
    "score = np.array([float(t.replace(\"tensor(\", \"\").replace(\")\", \"\")) for t in score[0].values])\n",
    "ugreen = np.array([float(t.replace(\"tensor(\", \"\").replace(\")\", \"\")) for t in ugreen[0].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_lexicalsim = pd.read_csv('/restricted/projectnb/batmanlab/chyuwang/rrg_factual_uncertainty/exp_result/ChexpertPlus/chexpert-plus_lexicalUQ.csv')\n",
    "u_lexicalsim = u_lexicalsim['ROUGE_L_UQ'].values\n",
    "u_lexicalsim = [1-i for i in u_lexicalsim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainty_list = [u_lexicalsim,ugreen] # Normalized Entropy - Predictive Entropy  - Lexical Similarity - VRO-GREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "RadCliQ = res['RadCliQ-v0'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030000000000000006\n",
      "0.025\n"
     ]
    }
   ],
   "source": [
    "for idx,i in enumerate(uncertainty_list):\n",
    "    print(cal_rce(i,-RadCliQ,num_bins=20)) # RadCliQ - lexical , ugreen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030000000000000006\n",
      "0.019999999999999997\n"
     ]
    }
   ],
   "source": [
    "for idx,i in enumerate(uncertainty_list):\n",
    "    print(cal_rce(i,score,num_bins=20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
