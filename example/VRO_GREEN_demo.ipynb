{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of VRO-GREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/neurips/.conda/envs/green/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from green_score import GREEN\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from green_score.utils import process_responses, make_prompt, tokenize_batch_as_chat, truncate_to_max_len\n",
    "pair_to_reward_dict = dict()\n",
    "class GREENModel(nn.Module):\n",
    "    \"\"\"\n",
    "    GREENModel is a neural network model for evaluating radiology reports.\n",
    "\n",
    "    Args:\n",
    "        cuda (bool): Whether to use CUDA for GPU acceleration.\n",
    "        model_id_or_path (str): Path or identifier of the pretrained model.\n",
    "        do_sample (bool): Whether to sample during generation.\n",
    "        batch_size (int): Batch size for processing.\n",
    "        return_0_if_no_green_score (bool): Whether to return 0 if no green score is found.\n",
    "\n",
    "    Attributes:\n",
    "        model: Pretrained model for causal language modeling.\n",
    "        tokenizer: Tokenizer associated with the model.\n",
    "        categories (list): List of evaluation categories.\n",
    "        sub_categories (list): List of subcategories for error evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            cuda,\n",
    "            model_id_or_path,\n",
    "            do_sample=False,\n",
    "            batch_size=4,\n",
    "            return_0_if_no_green_score=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cuda = cuda\n",
    "        self.do_sample = do_sample\n",
    "        self.batch_size = batch_size\n",
    "        self.return_0_if_no_green_score = return_0_if_no_green_score\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_id_or_path,\n",
    "            trust_remote_code=True,\n",
    "            device_map={\"\": \"cuda:{}\".format(torch.cuda.current_device())} if cuda else \"cpu\",\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        self.model.eval()\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            pretrained_model_name_or_path=model_id_or_path,\n",
    "            add_eos_token=True,\n",
    "            use_fast=True,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"left\",\n",
    "        )\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.chat_template = \"{% for message in messages %}\\n{% if message['from'] == 'human' %}\\n{{ '<|user|>\\n' + message['value'] + eos_token }}\\n{% elif message['from'] == 'system' %}\\n{{ '<|system|>\\n' + message['value'] + eos_token }}\\n{% elif message['from'] == 'gpt' %}\\n{{ '<|assistant|>\\n'  + message['value'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "        self.categories = [\n",
    "            \"Clinically Significant Errors\",\n",
    "            \"Clinically Insignificant Errors\",\n",
    "            \"Matched Findings\",\n",
    "        ]\n",
    "\n",
    "        self.sub_categories = [\n",
    "            \"(a) False report of a finding in the candidate\",\n",
    "            \"(b) Missing a finding present in the reference\",\n",
    "            \"(c) Misidentification of a finding's anatomic location/position\",\n",
    "            \"(d) Misassessment of the severity of a finding\",\n",
    "            \"(e) Mentioning a comparison that isn't in the reference\",\n",
    "            \"(f) Omitting a comparison detailing a change from a prior study\",\n",
    "        ]\n",
    "\n",
    "    def get_response(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Generates responses using the model and processes them.\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor): Input IDs for the model.\n",
    "            attention_mask (Tensor): Attention mask for the input IDs.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Processed response list and output IDs.\n",
    "        \"\"\"\n",
    "        outputs = self.model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            do_sample=self.do_sample,\n",
    "            max_length=2048,\n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "        )\n",
    "\n",
    "        responses = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        response_list = process_responses(responses)\n",
    "\n",
    "        return response_list, outputs\n",
    "\n",
    "    def parse_error_counts(self, text, category, for_reward=False):\n",
    "        \"\"\"\n",
    "        Parses error counts from the generated text for a specific category.\n",
    "\n",
    "        Args:\n",
    "            text (str): Text to parse for error counts.\n",
    "            category (str): Category of errors to parse.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Sum of counts and list of subcategory counts.\n",
    "        \"\"\"\n",
    "        if category not in self.categories:\n",
    "            raise ValueError(\n",
    "                f\"Category {category} is not a valid category. Please choose from {self.categories}.\"\n",
    "            )\n",
    "\n",
    "        pattern = rf\"\\[{category}\\]:\\s*(.*?)(?:\\n\\s*\\n|\\Z)\"\n",
    "        category_text = re.search(pattern, text, re.DOTALL)\n",
    "\n",
    "        sum_counts = 0\n",
    "        sub_counts = [0 for i in range(6)]\n",
    "\n",
    "        if not category_text:\n",
    "            if self.return_0_if_no_green_score:\n",
    "                return sum_counts, sub_counts\n",
    "            else:\n",
    "                return None, [None for i in range(6)]\n",
    "\n",
    "        if category_text.group(1).startswith(\"No\"):\n",
    "            return sum_counts, sub_counts\n",
    "\n",
    "        if category == \"Matched Findings\":\n",
    "            counts = re.findall(r\"^\\b\\d+\\b(?=\\.)\", category_text.group(1))\n",
    "            if len(counts) > 0:\n",
    "                sum_counts = int(counts[0])\n",
    "            return sum_counts, sub_counts\n",
    "\n",
    "        else:\n",
    "            sub_categories = [s.split(\" \", 1)[0] + \" \" for s in self.sub_categories]\n",
    "            matches = sorted(re.findall(r\"\\([a-f]\\) .*\", category_text.group(1)))\n",
    "\n",
    "            if len(matches) == 0:\n",
    "                matches = sorted(re.findall(r\"\\([1-6]\\) .*\", category_text.group(1)))\n",
    "                sub_categories = [\n",
    "                    f\"({i})\" + \" \" for i in range(1, len(self.sub_categories) + 1)\n",
    "                ]\n",
    "\n",
    "            for position, sub_category in enumerate(sub_categories):\n",
    "                for match in range(len(matches)):\n",
    "                    if matches[match].startswith(sub_category):\n",
    "                        count = re.findall(r\"(?<=: )\\b\\d+\\b(?=\\.)\", matches[match])\n",
    "                        if len(count) > 0:\n",
    "                            sub_counts[position] = int(count[0])\n",
    "            return sum(sub_counts), sub_counts\n",
    "\n",
    "    def compute_green(self, response):\n",
    "        \"\"\"\n",
    "        Computes the green score based on significant clinical errors and matched findings.\n",
    "\n",
    "        Args:\n",
    "            response (str): Generated response to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            float: Computed green score.\n",
    "        \"\"\"\n",
    "        sig_present, sig_errors = self.parse_error_counts(response, self.categories[0])\n",
    "        matched_findings, _ = self.parse_error_counts(response, self.categories[2])\n",
    "\n",
    "        if matched_findings == 0:\n",
    "            return 0\n",
    "\n",
    "        if sig_present is None or matched_findings is None:\n",
    "            return None\n",
    "\n",
    "        return matched_findings / (matched_findings + sum(sig_errors))\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the model, computing green scores for input batch.\n",
    "\n",
    "        Args:\n",
    "            input_ids (Tensor): Input IDs for the model.\n",
    "            attention_mask (Tensor): Attention mask for the input IDs.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tensor of green scores and output IDs.\n",
    "        \"\"\"\n",
    "        if self.cuda:\n",
    "            input_ids = input_ids.cuda()\n",
    "            attention_mask = attention_mask.cuda()\n",
    "\n",
    "        reward_model_responses, output_ids = self.get_response(input_ids, attention_mask)\n",
    "\n",
    "        greens = [self.compute_green(response) for response in reward_model_responses]\n",
    "        greens = [green for green in greens if green is not None]\n",
    "        return torch.tensor(greens, dtype=torch.float), output_ids\n",
    "class GREEN(nn.Module):\n",
    "    \"\"\"\n",
    "    GREEN is a wrapper model for GREENModel, handling batching and aggregation.\n",
    "\n",
    "    Args:\n",
    "        cuda (bool): Whether to use CUDA for GPU acceleration.\n",
    "\n",
    "    Attributes:\n",
    "        model: GREENModel instance for evaluation.\n",
    "        tokenizer: Tokenizer associated with the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cuda, max_len=200, **kwargs):\n",
    "        super().__init__()\n",
    "        self.cuda = cuda\n",
    "        self.max_len = max_len\n",
    "        self.model = GREENModel(cuda, **kwargs)\n",
    "        self.tokenizer = self.model.tokenizer\n",
    "        if self.cuda:\n",
    "            print(\"Using {} GPUs!\".format(torch.cuda.device_count()))\n",
    "            # self.model = torch.nn.DataParallel(self.model)\n",
    "\n",
    "    def forward(self, refs, hyps):\n",
    "        \"\"\"\n",
    "        Forward pass for the model, computing green scores for pairs of reference and hypothesis reports.\n",
    "\n",
    "        Args:\n",
    "            refs (list): List of reference reports.\n",
    "            hyps (list): List of hypothesis reports.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Mean green score, tensor of green scores, and list of processed responses.\n",
    "        \"\"\"\n",
    "        assert len(refs) == len(hyps)\n",
    "\n",
    "        refs = truncate_to_max_len(refs, self.max_len)\n",
    "        hyps = truncate_to_max_len(hyps, self.max_len)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pairs_to_process = []\n",
    "            final_scores = torch.zeros(len(refs))\n",
    "            output_ids_dict = {}\n",
    "\n",
    "            # Iterate over ref-hyp pairs and populate final_scores and pairs_to_process\n",
    "            for i, (ref, hyp) in enumerate(zip(refs, hyps)):\n",
    "                if (ref, hyp) in pair_to_reward_dict:\n",
    "                    final_scores[i], output_ids = pair_to_reward_dict[(ref, hyp)]\n",
    "                    output_ids_dict[i] = output_ids\n",
    "                else:\n",
    "                    pairs_to_process.append((ref, hyp, i))\n",
    "\n",
    "            if pairs_to_process:\n",
    "                batch = [make_prompt(ref, hyp) for ref, hyp, _ in pairs_to_process]\n",
    "                batch = [[{\"from\": \"human\", \"value\": prompt}, {\"from\": \"gpt\", \"value\": \"\"}] for prompt in batch]\n",
    "                batch = tokenize_batch_as_chat(self.tokenizer, batch)\n",
    "\n",
    "                greens_tensor, output_ids = self.model(batch['input_ids'], batch['attention_mask'])\n",
    "\n",
    "                if len(greens_tensor) == len(pairs_to_process):\n",
    "                    for (ref, hyp, idx), score, out_id in zip(pairs_to_process, greens_tensor, output_ids):\n",
    "                        pair_to_reward_dict[(ref, hyp)] = (score, out_id)\n",
    "                        final_scores[idx] = score\n",
    "                        output_ids_dict[idx] = out_id\n",
    "                else:\n",
    "                    print(\"An inconsistency was detected in processing pairs.\")\n",
    "\n",
    "            responses = [output_ids_dict[i] for i in range(len(refs))]\n",
    "            responses = self.tokenizer.batch_decode(responses, skip_special_tokens=True)\n",
    "\n",
    "            mean_green = final_scores.mean()\n",
    "            return mean_green, final_scores, process_responses(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 5 reference (original) reports\n",
    "with open(\"prediction_5.pkl\", \"rb\") as f:\n",
    "    refs = pickle.load(f)\n",
    "# Load 10 sampled (hypothesis) reports for each of the 5 original reports \n",
    "with open(\"samples_5.pkl\", \"rb\") as f:\n",
    "    hyps = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********Original report:********\n",
      "The patient is status post coronary artery bypass graft surgery.  The cardiac, mediastinal and hilar contours appear unchanged.  There is no pleural effusion or pneumothorax.  The lungs appear clear.  A nodular opacity projecting over the right upper lobe appears unchanged and may be a nipple shadow.  There has been no significant change.\n",
      "********Sampled reports:********\n",
      "The patient is status post sternotomy.  Allowing for differences in technique, the cardiac, mediastinal, and hilar contours appear unchanged.  The lungs appear clear.  There are no pleural effusions or pneumothorax.  There has been no significant change.  A previously described nodular opacity in the lingula appears to be a nipple shadow on this examination.\n",
      "The patient is status post coronary artery bypass graft surgery.  The cardiac, mediastinal and hilar contours appear unchanged.  There is no pleural effusion or pneumothorax.  A nodular density projecting over the left upper lobe is unchanged and may possibly represent a nipple shadow, although a lung nodule cannot be excluded.  Otherwise, the lungs appear clear.\n",
      "AP upright and lateral views of the chest were provided.  Midline sternotomy wires and mediastinal clips are again noted.  A subtle nodular density projecting over the lower lung fields just above the cardiac silhouette is unchanged and most compatible with a nipple shadow.  The previously noted left basal opacity, likely representing pneumonia, has resolved.  There is no definite sign of pulmonary edema, effusion, or pneumothorax.  The heart and mediastinal contour appear stable.  No acute bony abnormality.\n",
      "The patient is status post coronary artery bypass graft surgery.  The cardiac, mediastinal and hilar contours appear unchanged.  A linear density projecting over the left suprahilar region is unchanged and suggestive of scarring.  The lungs appear clear.  Lung volumes are low.  There is no pleural effusion or pneumothorax.  A 1.4-centimetre nodule projects over the left 7th rib: there is no apparent change.  Moderate degenerative changes along the mid thoracic spine appear similar.\n",
      "Faint nodular opacity projecting over the midline over the ___ anterior rib is likely a nipple shadow.  There is no evidence of pneumothorax or pleural effusion.  Cardiac size is normal.  There is no free air under the diaphragm.\n",
      "The patient is status post coronary artery bypass graft surgery.  The cardiac, mediastinal and hilar contours are unchanged.  The aorta is tortuous.  The heart is normal in size.  As seen previously, the central pulmonary vessels are mildly prominent but not engorged.  There is no pneumothorax or pleural effusion.  A tiny nodule is noted projecting over the lateral left mid lung, a new finding, suggesting a nonpalpable nodule.  It is only visualized on the frontal view and may represent lung inflammation versus a small nodule.\n",
      "The patient is status post TAVR with intact sternotomy wires.  Again, there are prominent interstitial markings, without focal consolidation, likely related to known metastatic disease.  No focal consolidations seen. The cardiomediastinal silhouette remains mildly enlarged, but stable compared to prior examination. There is no pleural effusion.\n",
      "Lung volumes are low, with persistent enlargement of the cardiomediastinal silhouette. Mild vascular congestion appears stable.  The lungs are clear without focal consolidation, pleural effusion, or pneumothorax. Known left lower lobe lesion is better assessed on recent chest CT.  A previously seen left lower lobe opacity is no longer apparent.  Sternal wires and mediastinal clips are stable from the most recent chest radiograph.\n",
      "A fiducial marker is again seen in the periphery of a mass in the right lower lobe, better seen on CT chest from ___.  The mass is smaller compared to the prior CT.  There is new mediastinal shift to the right with increased right paratracheal soft tissue, suggesting new volume loss.  There is no pleural effusion or pneumothorax.  The heart and mediastinal contours are normal. Atherosclerotic calcification of the aortic knob is now prominent The known T12 compression fracture is not visualized on this radiograph. The rib fractures on the left are again seen.\n",
      "The patient is status post sternotomy and probably coronary artery bypass graft surgery.  The cardiac, mediastinal and hilar contours appear stable.  There is mild relative elevation of the right hemidiaphragm, but mostly by three convexity along the right hemidiaphragm.  In the lateral view there is a suggestion of a posterior mid lung opacity and a vague nodule projecting over the base of the right upper lobe.  It is not clear if there exist shadows on the lateral view and, but this could be a focus of infection.  The chest is hyperinflated.\n"
     ]
    }
   ],
   "source": [
    "print(\"********Original report:********\" )\n",
    "print(refs[0])\n",
    "print(\"********Sampled reports:********\")\n",
    "for i in hyps[0]:\n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.79s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "model = GREEN(\n",
    "    model_id_or_path=\"StanfordAIMI/GREEN-radllama2-7b\",\n",
    "    do_sample=False,  # should be always False\n",
    "    batch_size=10,\n",
    "    return_0_if_no_green_score=True,\n",
    "    cuda=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green uncertainty for 0-th sample is 0.5963419675827026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green uncertainty for 1-th sample is 0.5768253803253174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green uncertainty for 2-th sample is 0.657012939453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green uncertainty for 3-th sample is 0.3086904287338257\n",
      "Green uncertainty for 4-th sample is 0.503333330154419\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(hyps)):\n",
    "    mean_green, greens, text = model([refs[i]] * len(hyps[i]), hyps[i])\n",
    "    print(f'Green uncertainty for {i}-th sample is {1 - mean_green}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
